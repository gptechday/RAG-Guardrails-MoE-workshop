{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ed60d2b8",
      "metadata": {
        "id": "ed60d2b8"
      },
      "source": [
        "# Lab 3 – Simulating a Mixture‑of‑Experts Router"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f10dce08",
      "metadata": {
        "id": "f10dce08"
      },
      "source": [
        "Route queries to specialized ‘experts’ using LangChain’s `RouterChain`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53624261",
      "metadata": {
        "id": "53624261"
      },
      "source": [
        "## Environment Setup\n",
        "Set the following environment variables before running this lab:\n",
        "\n",
        "| Variable | Purpose |\n",
        "|----------|---------|\n",
        "| `OPENAI_API_KEY` | Enables OpenAI models used by LangChain (`ChatOpenAI`). |\n",
        "\n",
        "Local:\n",
        "```bash\n",
        "export OPENAI_API_KEY=\"sk-...\"\n",
        "```\n",
        "Colab:\n",
        "```python\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = 'sk-...'\n",
        "```\n",
        "⚠️ **Never expose keys publicly.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5bd86db",
      "metadata": {
        "id": "b5bd86db"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.getenv('OPENAI_API_KEY'):\n",
        "    raise ValueError('OPENAI_API_KEY is not set.')\n",
        "print('OPENAI key loaded.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f7732be",
      "metadata": {
        "id": "5f7732be"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain --upgrade matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29f4aea6",
      "metadata": {
        "id": "29f4aea6"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMRouterChain\n",
        "from langchain.output_parsers import RouterOutputParser\n",
        "\n",
        "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
        "\n",
        "prompt_animals = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'You are an expert zoologist.'),\n",
        "    ('user', '{query}')\n",
        "])\n",
        "\n",
        "prompt_plants = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'You are a botany professor.'),\n",
        "    ('user', '{query}')\n",
        "])\n",
        "\n",
        "router_prompt = 'Return ANIMALS if the question is about animals, else PLANTS.'\n",
        "router = LLMRouterChain.from_llm(\n",
        "    llm=llm,\n",
        "    prompt=router_prompt,\n",
        "    output_parser=RouterOutputParser(choices=['ANIMALS','PLANTS'])\n",
        ")\n",
        "\n",
        "query = 'How do elephants regulate body temperature?'\n",
        "branch = router.run(query=query)\n",
        "chain = (prompt_animals if branch=='ANIMALS' else prompt_plants) | llm\n",
        "print(chain.invoke({'query': query}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "123544c8",
      "metadata": {
        "id": "123544c8"
      },
      "source": [
        "### ✏️ Exercises\n",
        "1. Add a third expert on *geography* and update the router.\n",
        "2. Measure latency of routing vs. a single dense model call.\n",
        "3. Discuss how this relates to sparse MoE layers in Mixtral."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8936b58",
      "metadata": {
        "id": "d8936b58"
      },
      "source": [
        "## 2. Simulated MoE Router Demo\n",
        "Following the slides, we'll build a router that picks between two experts (`animals`, `plants`) and optionally a third (`geography`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8ff46a0",
      "metadata": {
        "id": "f8ff46a0"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMRouterChain\n",
        "from langchain.output_parsers import RouterOutputParser\n",
        "\n",
        "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
        "\n",
        "prompt_animals = ChatPromptTemplate.from_messages([\n",
        "    ('system','You are an expert zoologist.'),\n",
        "    ('user','{query}')\n",
        "])\n",
        "prompt_plants = ChatPromptTemplate.from_messages([\n",
        "    ('system','You are an expert botanist.'),\n",
        "    ('user','{query}')\n",
        "])\n",
        "prompt_geo = ChatPromptTemplate.from_messages([\n",
        "    ('system','You are an expert geographer.'),\n",
        "    ('user','{query}')\n",
        "])\n",
        "\n",
        "router_prompt = (\n",
        "    'Classify the user question into one of: ANIMALS, PLANTS, GEO.\\n'\n",
        "    'Respond with the class name only.'\n",
        ")\n",
        "router = LLMRouterChain.from_llm(\n",
        "    llm=ChatOpenAI(model='gpt-3.5-turbo', temperature=0),\n",
        "    prompt=router_prompt,\n",
        "    output_parser=RouterOutputParser(choices=['ANIMALS','PLANTS','GEO'])\n",
        ")\n",
        "\n",
        "def router_call(query):\n",
        "    topic = router.run(query)\n",
        "    if topic=='ANIMALS':\n",
        "        chain = prompt_animals | llm\n",
        "    elif topic=='PLANTS':\n",
        "        chain = prompt_plants | llm\n",
        "    else:\n",
        "        chain = prompt_geo | llm\n",
        "    return topic, chain.invoke({'query':query})\n",
        "\n",
        "sample_qs = ['Why do dogs bark?', 'What causes leaf chlorosis?', 'What is the tallest mountain in Africa?']\n",
        "for q in sample_qs:\n",
        "    t, ans = router_call(q)\n",
        "    print(f'[{t}] {q} -> {ans[:80]}...\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1446987a",
      "metadata": {
        "id": "1446987a"
      },
      "source": [
        "## 3. Latency Benchmark vs Dense Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef204968",
      "metadata": {
        "id": "ef204968"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "dense_llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
        "\n",
        "def dense_call(q):\n",
        "    return dense_llm.invoke(q)\n",
        "\n",
        "q='Explain photosynthesis in simple terms.'\n",
        "t0=time.perf_counter(); _=dense_call(q); t_dense=time.perf_counter()-t0\n",
        "t0=time.perf_counter(); _=router_call(q); t_router=time.perf_counter()-t0\n",
        "print(f'Dense latency: {t_dense:.2f}s, Router: {t_router:.2f}s')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa615a35",
      "metadata": {
        "id": "aa615a35"
      },
      "source": [
        "## 4. Router Load‑Balancing Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6850f75",
      "metadata": {
        "id": "e6850f75"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "queries = [\n",
        "    'Tell me about the Amazon rainforest',\n",
        "    'Describe a tiger\\'s diet',\n",
        "    'How high is Mount Everest?',\n",
        "    'Why are roses red?',\n",
        "    'Explain canine behavior',\n",
        "    'What is the capital of Japan?'\n",
        "]\n",
        "counts={'ANIMALS':0,'PLANTS':0,'GEO':0}\n",
        "for q in queries:\n",
        "    t,_=router_call(q)\n",
        "    counts[t]+=1\n",
        "\n",
        "plt.bar(counts.keys(), counts.values())\n",
        "plt.title('Router Expert Utilization')\n",
        "plt.ylabel('# Queries')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7fa5541",
      "metadata": {
        "id": "f7fa5541"
      },
      "source": [
        "## ✏️ Exercises (Lab 3)\n",
        "1. **Add a Coding Expert** – Create a `coding` expert system prompt and modify the router to classify DEV questions.\n",
        "2. **Latency Profiling** – Measure latency for 20 diverse queries; plot boxplots comparing dense vs router.\n",
        "3. **Load‑Balancing Tuning** – Add noise to router softmax (temperature) and observe expert distribution changes.\n",
        "4. **Router Robustness Test** – Craft a prompt‑injection attempt to force the router to choose the wrong expert; patch the router prompt to defend.\n",
        "5. **Cost Estimation** – Use `tiktoken` or similar to approximate token usage difference between dense and router setups."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}