{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a78abafc",
      "metadata": {
        "id": "a78abafc"
      },
      "source": [
        "# Lab 2 – Guardrailing an LLM with Guardrails.ai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eabb1559",
      "metadata": {
        "id": "eabb1559"
      },
      "source": [
        "> Build input/output validators to filter toxic language and enforce JSON schema."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd72de45",
      "metadata": {
        "id": "bd72de45"
      },
      "source": [
        "## Environment Setup\n",
        "This lab requires API keys for your LLM provider and optional services.\n",
        "\n",
        "| Variable | Purpose |\n",
        "|----------|---------|\n",
        "| `OPENAI_API_KEY` | Access to OpenAI chat/completions for guardrail validation & LLM‑judge |\n",
        "| `GUARDRAILS_OPENAI_API_KEY` | (Optional) Same as above; Guardrails can read it automatically |\n",
        "| `PINECONE_API_KEY` | (Optional) Re‑use from Lab 1 if you want to store logs/vectors in Pinecone |\n",
        "\n",
        "Set these before starting the notebook:\n",
        "```bash\n",
        "export OPENAI_API_KEY=\"sk-...\"\n",
        "jupyter lab\n",
        "```\n",
        "In Colab use `os.environ['OPENAI_API_KEY']='sk-...'`.\n",
        "\n",
        "> **Security Tip**: Never commit keys to GitHub or share them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66e33865",
      "metadata": {
        "id": "66e33865"
      },
      "outputs": [],
      "source": [
        "# Sanity check for keys\n",
        "import os\n",
        "if not os.getenv('OPENAI_API_KEY'):\n",
        "    raise ValueError('OPENAI_API_KEY missing. Set it before running this lab.')\n",
        "print('OpenAI key detected.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d77d2b9",
      "metadata": {
        "id": "4d77d2b9"
      },
      "outputs": [],
      "source": [
        "!pip -q install guardrails-ai --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e24de657",
      "metadata": {
        "id": "e24de657"
      },
      "outputs": [],
      "source": [
        "from guardrails import Guard\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Pet(BaseModel):\n",
        "    pet_type: str = Field(description='Species of pet')\n",
        "    name: str = Field(description='A unique pet name')\n",
        "\n",
        "prompt = '''Suggest a pet and a unique name.\\n${gr.complete_json_suffix_v2}'''\n",
        "\n",
        "pet_guard = Guard.for_pydantic(output_class=Pet, prompt=prompt)\n",
        "resp, validated = pet_guard()\n",
        "print(validated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceecd2e8",
      "metadata": {
        "id": "ceecd2e8"
      },
      "outputs": [],
      "source": [
        "# Toxic language validator example\n",
        "!guardrails hub install hub://guardrails/toxic_language\n",
        "from guardrails.hub import ToxicLanguage\n",
        "tox = Guard().use(ToxicLanguage, threshold=0.5, validation_method='sentence', on_fail='exception')\n",
        "try:\n",
        "    tox.validate('You are stupid!')\n",
        "except Exception as e:\n",
        "    print('Blocked:', e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b1876ae",
      "metadata": {
        "id": "8b1876ae"
      },
      "source": [
        "### Integrate with RAG\n",
        "```python\n",
        "answer = qa_chain.run('Tell me something about Paris')\n",
        "clean = tox.validate(answer)\n",
        "print(clean)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b28f9f01",
      "metadata": {
        "id": "b28f9f01"
      },
      "source": [
        "## 3. Guardrail Examples & Workflows\n",
        "### 3.1 Prompt Engineering Guardrail (System Prompt)\n",
        "```python\n",
        "system_prompt = (\n",
        "    'You are a helpful assistant. Do not generate unsafe, unethical or biased text. '\n",
        "    'Follow the provided context closely.'\n",
        ")\n",
        "```\n",
        "### 3.2 Input Validation – Regex Profanity & PII\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75d6df46",
      "metadata": {
        "id": "75d6df46"
      },
      "outputs": [],
      "source": [
        "from guardrails import Guard, OnFailAction\n",
        "from guardrails.validators import RegexMatch\n",
        "from guardrails.hub import ToxicLanguage\n",
        "\n",
        "# Custom profanity word list regex\n",
        "profane_words = r'(?i)(foo|barbadword)'\n",
        "profanity_validator = RegexMatch(pattern=profane_words, on_fail=OnFailAction.EXCEPTION)\n",
        "\n",
        "# Guard with both profanity and toxicity validators\n",
        "guard_in = Guard().use_many(\n",
        "    profanity_validator,\n",
        "    ToxicLanguage(threshold=0.7, validation_method='sentence', on_fail=OnFailAction.EXCEPTION)\n",
        ")\n",
        "\n",
        "try:\n",
        "    guard_in.validate('You are a foo idiot!')\n",
        "except Exception as e:\n",
        "    print('Blocked:', e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7524e247",
      "metadata": {
        "id": "7524e247"
      },
      "source": [
        "### 3.3 Model‑Based Guardrail (LLM Judge)\n",
        "Use one LLM to grade another LLM’s answer for faithfulness / toxicity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1226cfd2",
      "metadata": {
        "id": "1226cfd2"
      },
      "outputs": [],
      "source": [
        "from guardrails.hub import ToxicLanguage\n",
        "from openai import OpenAI\n",
        "from guardrails import Guard\n",
        "\n",
        "client = OpenAI()\n",
        "judge_prompt = (\n",
        "    'You are an impartial safety grader. If the following answer contains hate, return \"unsafe\" else \"safe\".'\n",
        ")\n",
        "\n",
        "def llm_judge(text):\n",
        "    resp = client.chat.completions.create(\n",
        "        model='gpt-3.5-turbo',\n",
        "        messages=[{'role':'system','content':judge_prompt}, {'role':'user','content':text}]\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "# Wrap as guard\n",
        "class JudgeValidator:\n",
        "    def __call__(self, candidate):\n",
        "        verdict = llm_judge(candidate)\n",
        "        if verdict != 'safe':\n",
        "            raise ValueError('LLM Judge flagged unsafe content')\n",
        "\n",
        "guard_judge = Guard().use(JudgeValidator())\n",
        "guard_judge.validate('I hate all people of XYZ group.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e4a8e9d",
      "metadata": {
        "id": "2e4a8e9d"
      },
      "source": [
        "### 3.4 Latency Overhead Measurement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef883b47",
      "metadata": {
        "id": "ef883b47"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "text = 'Tell me a funny joke about cats.'\n",
        "\n",
        "start = time.perf_counter()\n",
        "raw = client.chat.completions.create(model='gpt-3.5-turbo', messages=[{'role':'user','content':text}])\n",
        "t_raw = time.perf_counter()-start\n",
        "\n",
        "start = time.perf_counter()\n",
        "guarded = guard_in.validate(text)\n",
        "t_guard = time.perf_counter()-start\n",
        "\n",
        "print(f'No guard latency: {t_raw:.2f}s, With guard: {t_guard:.2f}s')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aa47fda",
      "metadata": {
        "id": "5aa47fda"
      },
      "source": [
        "## ✏️ Exercises (Lab 2)\n",
        "1. **Custom JSON Schema** – Modify the `Pet` example to enforce nested address fields.\n",
        "2. **Profanity & PII Regex** – Extend the regex list; test with edge‑cases.\n",
        "3. **LLM‑Judge Faithfulness** – Adapt `JudgeValidator` to grade grounding given retrieved context from Lab 1.\n",
        "4. **Latency Benchmark** – Run the timing cell with and without `guard_in`; record overhead.\n",
        "5. **Red Team Mini‑Challenge (Bonus)** – Try to jailbreak the prompt guard; share successful attacks with your colleagues."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}