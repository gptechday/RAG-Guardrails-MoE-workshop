{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e1ca1831",
      "metadata": {
        "id": "e1ca1831"
      },
      "source": [
        "# Lab 1 — Retrieval‑Augmented Generation (RAG)\n",
        "**Goal:** Build and experiment with a RAG pipeline that matches all concepts covered in the slides.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71b91ea4",
      "metadata": {
        "id": "71b91ea4"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain transformers sentence-transformers datasets faiss-cpu pinecone-client rank_bm25 nltk\n",
        "import nltk, os, warnings\n",
        "nltk.download('punkt', quiet=True)\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98e40ac9",
      "metadata": {
        "id": "98e40ac9"
      },
      "source": [
        "## Environment Setup\n",
        "Before running this lab you’ll need a few **environment variables** for API access:\n",
        "\n",
        "| Variable | Purpose |\n",
        "|----------|---------|\n",
        "| `OPENAI_API_KEY` | Access to OpenAI models (used by LangChain’s `OpenAI()` wrapper) |\n",
        "| `PINECONE_API_KEY` | (Optional) Auth token for Pinecone vector DB |\n",
        "| `PINECONE_ENV` | (Optional) Your Pinecone environment region, e.g. `us-east-1-gcp` |\n",
        "\n",
        "On a local machine you can set them in a terminal **before** launching Jupyter:\n",
        "```bash\n",
        "export OPENAI_API_KEY=\"sk-...\"\n",
        "export PINECONE_API_KEY=\"your-pinecone-key\"\n",
        "export PINECONE_ENV=\"us-east-1-gcp\"\n",
        "jupyter lab\n",
        "```\n",
        "In Google Colab, use the Secrets tool or run:\n",
        "```python\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = 'sk-...'\n",
        "```\n",
        "\n",
        "> ⚠️ **Never commit keys to version control or share them publicly.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7866a2c4",
      "metadata": {
        "id": "7866a2c4"
      },
      "outputs": [],
      "source": [
        "# ✅ Quick sanity‑check for required keys\n",
        "import os, openai\n",
        "if not os.getenv('OPENAI_API_KEY'):\n",
        "    raise ValueError('OPENAI_API_KEY not set. Please set it before running the lab.')\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "\n",
        "if os.getenv('PINECONE_API_KEY'):\n",
        "    print('Pinecone key detected — cloud retrieval enabled.')\n",
        "else:\n",
        "    print('Pinecone key not set — using local FAISS only.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e50e1508",
      "metadata": {
        "id": "e50e1508"
      },
      "source": [
        "## 0. Parameters\n",
        "Feel free to tweak these as you work through the exercises."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c690c6e",
      "metadata": {
        "id": "7c690c6e"
      },
      "outputs": [],
      "source": [
        "CHUNK_SIZE = 512          # tokens per chunk (Exercise 1)\n",
        "TOP_K = 5                 # retrieved docs\n",
        "HYBRID_ALPHA = 0.5        # 0=dense only, 1=sparse only, blend for hybrid\n",
        "PINECONE_INDEX = 'rag-demo-' + ''.join(__import__('random').choices('abcdefghijklmnopqrstuvwxyz0123456789', k=6))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abcbc153",
      "metadata": {
        "id": "abcbc153"
      },
      "source": [
        "## 1. Load a Mini Corpus\n",
        "We'll use 1‑% of English Wikipedia for speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8266ff7d",
      "metadata": {
        "id": "8266ff7d"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "raw = load_dataset('wikipedia', '20220301.en', split='train[:1%]')\n",
        "docs = [t[:3000] for t in raw['text'][:500]]  # limit to 500 docs\n",
        "print(f'Loaded {len(docs)} documents')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1baf7036",
      "metadata": {
        "id": "1baf7036"
      },
      "source": [
        "## 2. Chunk, Embed, and Build Vector Stores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb423c46",
      "metadata": {
        "id": "fb423c46"
      },
      "outputs": [],
      "source": [
        "# 2.1 Chunk\n",
        "from nltk import sent_tokenize\n",
        "def chunk_text(text, chunk_size=CHUNK_SIZE):\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks, current = [], ''\n",
        "    for sent in sentences:\n",
        "        if len(current.split()) + len(sent.split()) < chunk_size:\n",
        "            current += ' ' + sent\n",
        "        else:\n",
        "            chunks.append(current.strip())\n",
        "            current = sent\n",
        "    if current:\n",
        "        chunks.append(current.strip())\n",
        "    return chunks\n",
        "\n",
        "chunks = []\n",
        "for d in docs:\n",
        "    chunks.extend(chunk_text(d))\n",
        "print(f'Created {len(chunks)} chunks (avg {sum(len(c.split()) for c in chunks)//len(chunks)} tokens)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d4158ec",
      "metadata": {
        "id": "0d4158ec"
      },
      "outputs": [],
      "source": [
        "# 2.2 Embeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e2e001e",
      "metadata": {
        "id": "0e2e001e"
      },
      "outputs": [],
      "source": [
        "# 2.3 Local FAISS Vector DB\n",
        "from langchain.vectorstores import FAISS\n",
        "faiss_store = FAISS.from_texts(chunks, embedding_model)\n",
        "retriever_dense = faiss_store.as_retriever(search_kwargs={'k': TOP_K})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50375ef6",
      "metadata": {
        "id": "50375ef6"
      },
      "outputs": [],
      "source": [
        "# 2.4 BM25 Sparse Retriever for Hybrid Search\n",
        "from rank_bm25 import BM25Okapi\n",
        "tokenized = [doc.split() for doc in chunks]\n",
        "bm25 = BM25Okapi(tokenized)\n",
        "\n",
        "class BM25Retriever:\n",
        "    def __init__(self, k=TOP_K):\n",
        "        self.k = k\n",
        "    def get_relevant_documents(self, query):\n",
        "        scores = bm25.get_scores(query.split())\n",
        "        top_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:self.k]\n",
        "        return [{'page_content': chunks[i], 'score': scores[i]} for i in top_idx]\n",
        "\n",
        "retriever_sparse = BM25Retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2fc3ac2",
      "metadata": {
        "id": "f2fc3ac2"
      },
      "outputs": [],
      "source": [
        "# 2.5 Hybrid Retriever (Dense + Sparse Blend)\n",
        "def hybrid_search(query, alpha=HYBRID_ALPHA):\n",
        "    dense_hits = retriever_dense.get_relevant_documents(query)\n",
        "    sparse_hits = retriever_sparse.get_relevant_documents(query)\n",
        "    score_dict = {}\n",
        "    for h in dense_hits:\n",
        "        score_dict[h['page_content']] = score_dict.get(h['page_content'], 0) + (1-alpha)*h['score']\n",
        "    for h in sparse_hits:\n",
        "        score_dict[h['page_content']] = score_dict.get(h['page_content'], 0) + alpha*h['score']\n",
        "    ranked = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)[:TOP_K]\n",
        "    return [{'page_content': t[0], 'score': t[1]} for t in ranked]\n",
        "\n",
        "class HybridRetriever:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def get_relevant_documents(self, query):\n",
        "        return hybrid_search(query)\n",
        "\n",
        "retriever_hybrid = HybridRetriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6b69f59",
      "metadata": {
        "id": "f6b69f59"
      },
      "source": [
        "### Optional: Pinecone Cloud Vector DB\n",
        "Set `PINECONE_API_KEY` and `PINECONE_ENV`, then run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "556fdbf5",
      "metadata": {
        "id": "556fdbf5"
      },
      "outputs": [],
      "source": [
        "import os, pinecone\n",
        "from langchain.vectorstores import Pinecone\n",
        "if os.getenv('PINECONE_API_KEY'):\n",
        "    pinecone.init(api_key=os.getenv('PINECONE_API_KEY'), environment=os.getenv('PINECONE_ENV', 'us-east-1-gcp'))\n",
        "    if PINECONE_INDEX not in pinecone.list_indexes():\n",
        "        pinecone.create_index(PINECONE_INDEX, dimension=384, metric='cosine')\n",
        "    index = pinecone.Index(PINECONE_INDEX)\n",
        "    pc_store = Pinecone(index, embedding_model.embed_query, embedding_model.embed_documents)\n",
        "    retriever_pine = pc_store.as_retriever(search_kwargs={'k': TOP_K})\n",
        "else:\n",
        "    print('Pinecone environment variables not set; skipping cloud retriever.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00c6c1be",
      "metadata": {
        "id": "00c6c1be"
      },
      "source": [
        "## 3. Instantiate an LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48665268",
      "metadata": {
        "id": "48665268"
      },
      "outputs": [],
      "source": [
        "from langchain_community.llms import OpenAI\n",
        "llm = OpenAI(model='gpt-3.5-turbo', temperature=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "230e7007",
      "metadata": {
        "id": "230e7007"
      },
      "source": [
        "## 4. Build RetrievalQA Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0298633f",
      "metadata": {
        "id": "0298633f"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "qa_dense = RetrievalQA.from_llm(llm=llm, retriever=retriever_dense)\n",
        "qa_hybrid = RetrievalQA.from_llm(llm=llm, retriever=retriever_hybrid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e7878aa",
      "metadata": {
        "id": "5e7878aa"
      },
      "outputs": [],
      "source": [
        "# 4.1 Conversational RAG\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "conv_chain = ConversationalRetrievalChain.from_llm(llm, retriever_dense)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb49593d",
      "metadata": {
        "id": "cb49593d"
      },
      "source": [
        "## 5. Quick Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95f9ecf4",
      "metadata": {
        "id": "95f9ecf4"
      },
      "outputs": [],
      "source": [
        "question = 'What is the capital of France and why is it significant?'\n",
        "print('Dense answer:')\n",
        "print(qa_dense.run(question))\n",
        "\n",
        "print('\\nHybrid answer:')\n",
        "print(qa_hybrid.run(question))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ea076be",
      "metadata": {
        "id": "7ea076be"
      },
      "source": [
        "## ✏️ Exercises (Match Slides)\n",
        "1. **Chunk‑Size Sensitivity**  – Change `CHUNK_SIZE` to 256 and 768, rebuild chunks & FAISS, and compare answer grounding quality.\n",
        "2. **Hybrid Search** – Adjust `HYBRID_ALPHA` (0.0, 0.5, 1.0). Which blend retrieves best context for numeric, keyword‑heavy queries?\n",
        "3. **Conversational RAG** – Use `conv_chain` to ask a follow‑up: “How many people live there?” after the capital question.\n",
        "4. **💠 Pinecone Cloud** – Enable Pinecone and time latency vs FAISS. Report recall and speed differences.\n",
        "5. **Grounding Check (Bonus)** – Write a simple function that verifies each sentence in the answer exists in at least one retrieved chunk."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}