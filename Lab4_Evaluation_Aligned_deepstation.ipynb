{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1f7d5547",
      "metadata": {
        "id": "1f7d5547"
      },
      "source": [
        "# Lab 4 – Evaluating RAG & Guardrails"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90ed16d0",
      "metadata": {
        "id": "90ed16d0"
      },
      "source": [
        "Compute precision/recall of retrieved docs, guardrail false‑positive rates, and MoE latency."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b37dc9f8",
      "metadata": {
        "id": "b37dc9f8"
      },
      "source": [
        "## Environment Setup\n",
        "Set `OPENAI_API_KEY` for metric computations that rely on LLM calls (RAGAS & LLM‑judge).\n",
        "```bash\n",
        "export OPENAI_API_KEY=\"sk-...\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d5ec570",
      "metadata": {
        "id": "2d5ec570"
      },
      "outputs": [],
      "source": [
        "!pip -q install ragas scikit-learn matplotlib tqdm openai guardrails-ai\n",
        "\n",
        "import os, openai, warnings\n",
        "if not os.getenv('OPENAI_API_KEY'):\n",
        "    raise ValueError('OPENAI_API_KEY missing; metrics requiring LLM will fail.')\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59a4efa2",
      "metadata": {
        "id": "59a4efa2"
      },
      "outputs": [],
      "source": [
        "!pip -q install ragas --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e74895ca",
      "metadata": {
        "id": "e74895ca"
      },
      "outputs": [],
      "source": [
        "# Example evaluation of RAG with RAGAS (placeholder – requires data)\n",
        "from ragas import evaluate\n",
        "# Suppose we have a dataset of (question, ground_truth_answer)\n",
        "# ragas needs predictions + retrieved docs; refer to docs for full pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02c5f5fb",
      "metadata": {
        "id": "02c5f5fb"
      },
      "outputs": [],
      "source": [
        "# Timing router vs single model\n",
        "import time, functools\n",
        "start = time.perf_counter()\n",
        "_ = chain.invoke({'query': query})\n",
        "print('Router latency:', time.perf_counter()-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65842766",
      "metadata": {
        "id": "65842766"
      },
      "source": [
        "### ✏️ Exercises\n",
        "1. Build a small dataset of 10 Q/A pairs and evaluate your RAG.\n",
        "2. Compute guardrail block rate on a provided list of toxic prompts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a053e7c",
      "metadata": {
        "id": "3a053e7c"
      },
      "source": [
        "## 1. Prepare Evaluation Dataset\n",
        "Use the RAG chain from Lab 1 to answer a small set of questions and store references."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bf46bfe",
      "metadata": {
        "id": "7bf46bfe"
      },
      "outputs": [],
      "source": [
        "qa_chain = None  # TODO: load from Lab1 or rebuild quickly\n",
        "questions = [\n",
        "    'What is the tallest mountain in Africa?',\n",
        "    'Who wrote the novel 1984?',\n",
        "    'When was the Eiffel Tower built?',\n",
        "    'Define photosynthesis.',\n",
        "    'What currency is used in Japan?'\n",
        "]\n",
        "preds, contexts, gts = [], [], []\n",
        "for q in questions:\n",
        "    ans = qa_chain.run(q)\n",
        "    preds.append(ans)\n",
        "    contexts.append(qa_chain._last_retrieved_docs)\n",
        "    gts.append('')  # put ground truth manually or file\n",
        "print('Collected', len(preds), 'QA pairs')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdfc8f73",
      "metadata": {
        "id": "fdfc8f73"
      },
      "source": [
        "## 2. RAG Quality Metrics with RAGAS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ead418b",
      "metadata": {
        "id": "8ead418b"
      },
      "outputs": [],
      "source": [
        "from ragas import evaluate, metrics\n",
        "df = evaluate(\n",
        "    questions=questions,\n",
        "    answers=preds,\n",
        "    contexts=[[d.page_content for d in ctx] for ctx in contexts],\n",
        "    metrics=[metrics.precision, metrics.recall, metrics.faithfulness]\n",
        ")\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6663359",
      "metadata": {
        "id": "f6663359"
      },
      "source": [
        "## 3. Guardrail False‑Positive / False‑Negative Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef7f7877",
      "metadata": {
        "id": "ef7f7877"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "toxic_samples = ['You are stupid!', 'Great work!']\n",
        "labels = [1,0]  # 1=toxic\n",
        "preds_guard = []\n",
        "for s in toxic_samples:\n",
        "    try:\n",
        "        guard_in.validate(s)\n",
        "        preds_guard.append(0)\n",
        "    except:\n",
        "        preds_guard.append(1)\n",
        "cm = confusion_matrix(labels, preds_guard)\n",
        "cm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a9617f2",
      "metadata": {
        "id": "0a9617f2"
      },
      "source": [
        "## 4. Latency & Cost Analysis for MoE Router"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4a63b53",
      "metadata": {
        "id": "a4a63b53"
      },
      "outputs": [],
      "source": [
        "import time, statistics, matplotlib.pyplot as plt\n",
        "qset = questions*3\n",
        "t_dense, t_router = [], []\n",
        "for q in qset:\n",
        "    start=time.perf_counter(); _=dense_call(q); t_dense.append(time.perf_counter()-start)\n",
        "    start=time.perf_counter(); _=router_call(q); t_router.append(time.perf_counter()-start)\n",
        "print('Median dense', statistics.median(t_dense), 'router', statistics.median(t_router))\n",
        "plt.boxplot([t_dense, t_router], labels=['Dense','Router'])\n",
        "plt.ylabel('seconds')\n",
        "plt.title('Latency Comparison')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7263179",
      "metadata": {
        "id": "b7263179"
      },
      "source": [
        "## ✏️ Exercises (Lab 4)\n",
        "1. Expand the evaluation dataset to 20 Q/A pairs; compute precision & recall.\n",
        "2. Implement a grounding checker that counts sentences not present in context.\n",
        "3. Plot confusion matrix for guardrail results using `seaborn.heatmap`.\n",
        "4. Evaluate token costs for dense vs router calls using `tiktoken`.\n",
        "5. **Stretch**: Compare faithfulness metric before and after increasing `TOP_K` in retrieval."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}